# MNIST 분류 실험 결과

## 기본 모델 성능

* 최종 테스트 정확도: **96.89%**
* 훈련 시간: **약 1분 30초**

---

## 실험 결과

### 실험 1: 학습률 변경

* **변경사항**:
  학습률을 `1e-4`, `3e-4`, `1e-3`, `3e-3`, `1e-2`로 각각 변경하여 실험
* **결과**:

  * 1e-4 → 훈련 91.91%, 테스트 92.67%
  * 3e-4 → 훈련 94.98%, 테스트 95.60%
  * 1e-3 → 훈련 97.06%, 테스트 96.89%
  * 3e-3 → 훈련 97.35%, 테스트 96.76%
  * 1e-2 → 훈련 95.80%, 테스트 95.47%
* **분석**:
  학습률이 너무 낮으면 수렴이 느려지고,
  너무 높으면 진동 및 과적합 발생.
  **1e-3**이 훈련 및 테스트 모두에서 가장 안정적 성능을 보임.

---

### 실험 2: 은닉층 크기 변경

* **변경사항**:
  은닉층 크기를 `50`, `100`, `200`으로 변경
* **결과**:

  * 50 → 훈련 95.87%, 테스트 96.14%
  * 100 → 훈련 97.06%, 테스트 96.89%
  * 200 → 훈련 97.59%, 테스트 97.52%
* **분석**:
  은닉층 크기를 늘릴수록 표현력이 향상되어 정확도 상승.
  `hidden_size=200`이 가장 높은 성능을 보였으나,
  연산량 증가로 효율성 고려 필요.

---

### 실험 3: Epoch 변경

* **변경사항**:
  Epoch 수를 `3`, `5`, `10`으로 변경
* **결과**:

  * 3 → 훈련 97.06%, 테스트 96.89%
  * 5 → 훈련 98.11%, 테스트 97.64%
  * 10 → 훈련 99.20%, 테스트 97.52%
* **분석**:
  Epoch가 많아질수록 훈련 정확도는 향상되지만,
  **과적합**이 증가하는 경향을 보임.
  **Epoch=5**에서 효율적인 성능과 일반화 유지.

---

### 실험 4: 은닉층 수 변경

* **변경사항**:
  은닉층 수를 `1개`, `2개`, `3개`로 변경
* **결과**:

  * 1개 → 훈련 97.06%, 테스트 96.89%
  * 2개 → 훈련 99.05%, 테스트 98.01%
  * 3개 → 훈련 99.12%, 테스트 97.85%
* **분석**:
  은닉층 수가 증가할수록 훈련 정확도는 상승하지만
  테스트 정확도는 포화 상태에 도달하며,
  과적합 발생 가능성 존재.

---

## 결론 및 인사이트

* **가장 효과적인 개선 방법**:
  `학습률 1e-3`, `은닉층 크기 200`, `Epoch 5`
  → 훈련 정확도 **97.6%**, 테스트 정확도 **97.5%**, 과적합 거의 없음

* **관찰된 패턴**:

  * 학습률은 **1e-3**일 때 가장 안정적
  * 은닉층 크기 및 수 증가 시 훈련 정확도 향상
  * Epoch 증가 시 과적합 경향 존재

* **추가 개선 아이디어**:

  * Dropout 추가로 과적합 완화
---

## 클래스별 정확도 (3층 구조 기준)

* class 0: 99.39%

* class 1: 99.30%

* class 2: 89.73%

* class 3: 98.12%

* class 4: 95.62%

* class 5: 92.83%

* class 6: 97.49%

* class 7: 98.05%

* class 8: 84.29%

* class 9: 97.13%

* **매크로 평균 정확도**: 95.19%

* **자주 혼동되는 클래스**: (8, 3), (2, 7), (5, 3)
